---
title: "handout 2 - Xarxes Socials"
author: "Guillermo Gavilla Hernández"
date: "19/11/2020"
output: html_document
---


```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.align="center", echo=TRUE, warning=FALSE, message=FALSE,autodep = TRUE,cache=TRUE)
library(knitr)
library(printr)
library(igraph)
library(sna)
```

We shall consider again the undirected Facebook friendship network considered in the last handout. The links in this network are contained in the file **facebook_sample_anon.txt**. Download it on your computer and upload it to R as a dataframe. Define an undirected graph with this list of edges.



# Undirected Graph.

```{r read table }
data <- read.table("data/facebook_sample_anon.txt")
tail(data)
```
*Load data*

```{r generate a graph}
graph <- graph_from_data_frame(data, directed = F)
# plot(graph, edge.color = "darkgoldenrod1", edge.arrow.size = 0.001, edge.arrow.width = 1,
#     vertex.color = "cyan3", vertex.label= NA,
#     vertex.size = 12, rescale = T, main = "Undirected Graph from Facebook dataset")
graph

```


**1)**  It has been observed in many networks an association between "centrality" and "lethality," defined as the fatal disconnection of the network when nodes are removed. Let's study this association on this network.

#### Introducción "Conectividad"

```{r adjacency matrix }

#He creado una matriz de adjacencia para poder sacar el "connectedness score"

adj <- as_adjacency_matrix(graph, type = c("both", "upper", "lower"),
  attr = NULL, edges = FALSE, names = TRUE,
  sparse = F)

#print(paste(adj))

connectedness(adj)

# Observamos que el grado de connectividad es muy elevado ( =1 ). Por lo que podemos decir que     # tiene una conectividad perfecta.

```


```{r proba amb matriu de adjacencia}

# Hago una prueba para saber si esta bien,  quitando 2000 nodos a nuestro graph.

graph1 <- delete_vertices(graph, sample(4000,2000))

adj1 <- as_adjacency_matrix(graph1, type = c("both", "upper", "lower"),
  attr = NULL, edges = FALSE, names = TRUE,
  sparse = F)


(connectedness(adj1))

# La connectividad cae significativamente. ( =~ 0.5). Observamos que al quitar muchos nodos se
# reduce muchisimo las maneras de movernos en torno a los diferentes nodos de nuestro grafo.
# De esto podriamos deducir que podria haber una desconexion fatal al eliminar nodos.


```

```{r }

detach("package:sna", unload = T)



```




*a)* Repeat 1000 times the procedure of removing a random 0.1% of its set of nodes, and compute the average number of connected components of the resulting networks and the average fraction of the network represented by the largest component. Use set.seed to make your results reproducible.


We are going to set a seed to the random number generator to be able to reproduce our results.
```{r set seed}
set.seed(728754898)
```

```{r remove random N vertex function}
removeRandomNVertex <- function (graph, numberOfVertexToRemove) {
  result <- graph
  vertexCount <- vcount(facenet)
  if(vertexCount > 0 && numberOfVertexToRemove > 0 && numberOfVertexToRemove <= vertexCount) {
    randomIndex <- round(runif(numberOfVertexToRemove, 1, vertexCount), digits = 0)
    result <- delete_vertices(graph, randomIndex)
  } else {
    print("Your graph is empty")
  }
  result
}
```

First of all, let's get reference value of the unmodified *facenet*
```{r reference components}
components <- components(facenet)
```
```{r print_reference_components, echo = FALSE}
print(paste("facenet has", components$no, "component", sep = " "))
print(paste("this component is of size", components$csize, sep = " "))
```

Now we are going to repeat 1000 times a removal of 0.1% of vertex and check how many components we have. Then print the average.
```{r remove random 0.1% vertex}
numberOfRounds <- 1000
numberConnectedComponentsList <- NULL
fractionRepresentedByLargestComponentList <- NULL
nVertex <- vcount(facenet)
Op1PercentVertex <- round(nVertex * 0.001, digits = 0)
nVertexLeft <- nVertex - Op1PercentVertex
print(paste0("the 0.1% of vertex is ", Op1PercentVertex,
             ". Then, we are going to remove ", Op1PercentVertex, " random nodes each loop"))
for(i in 1:numberOfRounds) {
  auxComponents <- components(removeRandomNVertex(facenet, Op1PercentVertex))
  numberConnectedComponentsList <- append(numberConnectedComponentsList,
                                          auxComponents$no)
  fractionRepresentedByLargestComponentList <- append(fractionRepresentedByLargestComponentList,
           max(auxComponents$csize)/nVertexLeft)
}
averageConnectedComponents <- mean(numberConnectedComponentsList)
averageFractionRepresentedByLargestComponent <- mean(fractionRepresentedByLargestComponentList)

print(paste0("the average fraction of the network represented by the largest component '",
             averageFractionRepresentedByLargestComponent, "'"))
```
After this test we can say that this network isn't that weak. Certainly has a weak point, but isn't something that by deleting some random vertex we can ensure is going to fall apart.


*b)* Now, compute the number of connected components and the fraction represented by the largest component of the networks obtained after removing the most central 0.1% of nodes, for the following centrality indices (of course, if the most central 0.1% of nodes for two indices are the same set of nodes, you need not waste your time considering twice the same network): *degree*; *closeness*; *betweenness*; *page.rank*. (**Hint**: It might be convenient to define first a function that removes a given set of nodes of this graph and computes the number of connected components, and the fraction represented by the largest component of the resulting network; then you will only need to apply it to the required different sets of most central nodes.) Is it what you expected?


```{r}

```

```{r}
centr.degree <- centr_degree(facenet)
centr.degree$centralization
centr.degree$theoretical_max
```

```{r}
centr.betw <- centr_betw(facenet)
centr.betw$centralization
centr.betw$theoretical_max
```
```{r}
centr.clo <- centr_clo(facenet)
centr.clo$centralization
centr.clo$theoretical_max
```


Let's remove the 0.1% of the vertex with higher **degrees** and check how many components we have.
```{r degrees}
vertexWithHighestDegrees <- names(sort(degree(facenet), decreasing = TRUE)[1:Op1PercentVertex])
vertexWithHighestDegrees
components.degrees <- components(delete_vertices(facenet, vertexWithHighestDegrees))

nVertex <- vcount(facenet) - Op1PercentVertex
components.degrees.no <- components.degrees$no
components.degrees.max <- max(components.degrees$csize / nVertex)
```
We can see that by removing the 0.1% of nodes with the highest degrees, "107", "1684", "1912" and "3437", we can split the network in 41 components.

Let's remove the 0.1% of the vertex with higher **closeness** and check how many components we have.
```{r closeness}
vertexWithHighestCentralCloseness <- names(sort(closeness(facenet), decreasing = TRUE)[1:Op1PercentVertex])
vertexWithHighestCentralCloseness
```
Now compare the 0.1% highest closeness with the list in degrees.
```{r closeness2}
vertexWithHighestDegrees %in% vertexWithHighestCentralCloseness
```
The
```{r closeness3}
components(delete_vertices(facenet, vertexWithHighestCentralCloseness))$no
```
This time by removing vertex "107", "58", "428" and "563" with higher closeness have result in fewer components, 12.

Let's remove the 0.1% of the vertex with higher **betweenness** and check how many components we have.
```{r betweenness}
vertexWithHighestBetweenness <- names(sort(betweenness(facenet), decreasing = TRUE)[1:Op1PercentVertex])
vertexWithHighestBetweenness
```
Now compare the 0.1% highest betweenness with the list in degrees.
```{r betweenness2}
vertexWithHighestDegrees %in% vertexWithHighestBetweenness
# no need for this results, we already know its value
#vertexWithHighestCentralCloseness %in% vertexWithHighestBetweenness # false
#components(delete_vertices(facenet, vertexWithHighestBetweenness))$no # same as degrees, 41
```
Booth lists are equal, then betweenness result is expected to be equal to degrees, 41 components.

Let's remove the 0.1% of the vertex with higher **page_rank score** and check how many components we have.
```{r page_rank}
#names(sort(page_rank(facenet, algo = "power", directed = FALSE)$vector, decreasing = TRUE)[1:Op1PercentVertex]) makes klint to crash ...

# page rank with prpack algorithm
vertexWithHighestPageRank_prpack <- names(sort(page_rank(facenet, algo = "arpack", directed = FALSE)$vector, decreasing = TRUE)[1:Op1PercentVertex])

# page rank with arpack algorithm
vertexWithHighestPageRank_arpack <- names(sort(page_rank(facenet, algo = "prpack", directed = FALSE)$vector, decreasing = TRUE)[1:Op1PercentVertex])
vertexWithHighestPageRank
```
Now compare the 0.1% highest page_rank scores with the different algorithms, and then with degrees/betweenness and closeness.
```{r page_rank2}
# Compare lists using different algorithms
sort(vertexWithHighestPageRank_prpack) == sort(vertexWithHighestPageRank_arpack) # IF all TRUE booth lists are equal
# Equal to degrees and betweenness?
sort(vertexWithHighestDegrees) == sort(vertexWithHighestPageRank_arpack)
# Equal to closeness?
sort(vertexWithHighestCentralCloseness) == sort(vertexWithHighestPageRank_arpack)
```
There is no difference between algorithms within page_rank in this case. We can do only 1 components run with any of page_rank lists.
Page_rank also selected different vertex than the other 3 methods. Let's calculate its components.
```{r page_rank3}
components(delete_vertices(facenet, vertexWithHighestPageRank))$no
```
Page_rank scored the maximum components, 52. This means that page_rank is able to select the most valuable nodes in the newtwork.

To conclude, best was page_rank with 52 components, followed by degrees and betweenness with 41, and last closeness with 12.
I was expecting degrees to be the best one but page_rank proved to be better.

**2)** Now, consider the same graph as a directed one, and find the hubs and authorities scores. Compare with the page rank score.

```{r generate directed graph}

graph_directed <- graph_from_data_frame(data, directed = T)


```

```{r HUBS and AUTHORITIES}

hub_score <- hub_score(graph_directed, scale=T)$vector
authority_score <- authority_score(graph_directed, scale =T )$vector


par(mfrow=c(1,2))
set.seed(123)
plot(graph_directed,
     vertex.size=hub_score*30,
     main = "Hubs",
     vertex.color = rainbow(52),
     edge.arrow.size=0.1,
     layout = layout.kamada.kawai,
     vertex.label = NA)

plot(graph_directed,
     vertex.size=authority_score*30,
     main = "Authority",
     vertex.color = rainbow(52),
     edge.arrow.size=0.1,
     layout = layout.kamada.kawai,
     vertex.label = NA)


#Observamos en los gráficos como las zonas donde hay Hubs y Authority coinciden.


```

```{r Pagerank score}

page_rank <-page_rank(graph_directed)$vector

par(mfrow=c(1,2))
set.seed(123)

plot(graph_directed,
     vertex.size=page_rank*30,
     main = "Page rank",
     vertex.color = rainbow(52),
     edge.arrow.size=0.1,
     layout = layout.kamada.kawai,
     vertex.label = NA)

```

