---
title: "handout 2 - Xarxes Socials"
author: "Guillermo Gavilla Hernández and Miquel Antoni Llambías Cabot"
date: "19/11/2020"
output: html_document
---

* Name 1: Miquel Antoni Llambías Cabot
* Name 2: Guillermo Gavilla Hernández

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.align="center", echo=TRUE, warning=FALSE, message=FALSE,autodep = TRUE,cache=FALSE)
library(knitr)
library(printr)
library(igraph)
library(sna)
```

We shall consider again the undirected Facebook friendship network considered in the last handout. The links in this network are contained in the file **facebook_sample_anon.txt**. Download it on your computer and upload it to R as a dataframe. Define an undirected graph with this list of edges.



# Undirected Graph.

```{r read table }
data <- read.table("data/facebook_sample_anon.txt")
tail(data)
```
*Load data*

```{r generate a graph}
facenet <- graph_from_data_frame(data, directed = F)
# plot(graph, edge.color = "darkgoldenrod1", edge.arrow.size = 0.001, edge.arrow.width = 1,
#     vertex.color = "cyan3", vertex.label= NA,
#     vertex.size = 12, rescale = T, main = "Undirected Graph from Facebook dataset")
facenet
```


**1)**  It has been observed in many networks an association between "centrality" and "lethality," defined as the fatal disconnection of the network when nodes are removed. Let's study this association on this network.

#### Introducción "Conectividad"

```{r adjacency matrix }

#He creado una matriz de adjacencia para poder sacar el "connectedness score"

adj <- as_adjacency_matrix(facenet, type = c("both", "upper", "lower"),
                           attr = NULL, edges = FALSE, names = TRUE,
                           sparse = F)

#print(paste(adj))

connectedness(adj)

# Observamos que el grado de connectividad es muy elevado ( =1 ). Por lo que podemos decir que     # tiene una conectividad perfecta.

```


```{r proba amb matriu de adjacencia}

# Hago una prueba para saber si esta bien,  quitando 2000 nodos a nuestro graph.

graph1 <- delete_vertices(facenet, sample(4000, 2000))

adj1 <- as_adjacency_matrix(graph1, type = c("both", "upper", "lower"),
  attr = NULL, edges = FALSE, names = TRUE,
  sparse = F)


(connectedness(adj1))

# La connectividad cae significativamente. ( =~ 0.5). Observamos que al quitar muchos nodos se
# reduce muchisimo las maneras de movernos en torno a los diferentes nodos de nuestro grafo.
# De esto podriamos deducir que podria haber una desconexion fatal al eliminar nodos.


```

```{r }

detach("package:sna", unload = T)



```




*a)* Repeat 1000 times the procedure of removing a random 0.1% of its set of nodes, and compute the average number of connected components of the resulting networks and the average fraction of the network represented by the largest component. Use set.seed to make your results reproducible.


We are going to set a seed to the random number generator to be able to reproduce our results.
```{r set seed}
set.seed(728754898)
```

```{r remove random N vertex function}
removeRandomNVertex <- function (graph, numberOfVertexToRemove) {
  result <- graph
  vertexCount <- vcount(graph)
  if(vertexCount > 0 && numberOfVertexToRemove > 0 && numberOfVertexToRemove <= vertexCount) {
    randomIndex <- round(runif(numberOfVertexToRemove, 1, vertexCount), digits = 0)
    result <- delete_vertices(graph, randomIndex)
  } else {
    print("Your graph is empty")
  }
  result
}
```

First of all, let's get reference value of the unmodified *facenet*
```{r reference components}
components <- components(facenet)
```
```{r print_reference_components, echo = FALSE}
print(paste("facenet has", components$no, "component", sep = " "))
print(paste("this component is of size", components$csize, sep = " "))
```

Now we are going to repeat 1000 times a removal of 0.1% of vertex and check how many components we have. Then print the average.
```{r remove random 0.1% vertex}
numberOfRounds <- 1000
numberConnectedComponentsList <- NULL
fractionRepresentedByLargestComponentList <- NULL
nVertex <- vcount(facenet)
Op1PercentVertex <- round(nVertex * 0.001, digits = 0)
nVertexLeft <- nVertex - Op1PercentVertex
print(paste0("the 0.1% of vertex is ", Op1PercentVertex,
             ". Then, we are going to remove ", Op1PercentVertex, " random nodes each loop"))
for(i in 1:numberOfRounds) {
  auxComponents <- components(removeRandomNVertex(facenet, Op1PercentVertex))
  numberConnectedComponentsList <- append(numberConnectedComponentsList,
                                          auxComponents$no)
  fractionRepresentedByLargestComponentList <- append(fractionRepresentedByLargestComponentList,
           max(auxComponents$csize)/nVertexLeft)
}
averageConnectedComponents <- mean(numberConnectedComponentsList)
averageFractionRepresentedByLargestComponent <- mean(fractionRepresentedByLargestComponentList)

print(paste0("the average number of connected components after a removal of 0.1% of vertex is '",
             averageConnectedComponents,
             "' components"))
print(paste0("the average fraction of the network represented by the largest component '",
             averageFractionRepresentedByLargestComponent, "'"))
```
After this test we can say that this network isn't that weak. Certainly has a weak point, but isn't something that by deleting some random vertex we can ensure is going to fall apart.


*b)* Now, compute the number of connected components and the fraction represented by the largest component of the networks obtained after removing the most central 0.1% of nodes, for the following centrality indices (of course, if the most central 0.1% of nodes for two indices are the same set of nodes, you need not waste your time considering twice the same network): *degree*; *closeness*; *betweenness*; *page.rank*. (**Hint**: It might be convenient to define first a function that removes a given set of nodes of this graph and computes the number of connected components, and the fraction represented by the largest component of the resulting network; then you will only need to apply it to the required different sets of most central nodes.) Is it what you expected?


```{r}
vertexWithHighestPageRank_arpack <- page_rank(facenet, algo = "prpack", directed = FALSE)


centr.page_rank <- centralize(vertexWithHighestPageRank_arpack$vector,
                              theoretical.max = vertexWithHighestPageRank_arpack$value,
                              normalized = TRUE)
centr.page_rank
#centr.page_rank$centralization
#centr.page_rank$theoretical_max
```

```{r}
centr.degree <- centr_degree(facenet)
centr.degree$centralization
centr.degree$theoretical_max
```

```{r}
centr.betw <- centr_betw(facenet)
centr.betw$centralization
centr.betw$theoretical_max
```
```{r}
centr.clo <- centr_clo(facenet)
centr.clo$centralization
centr.clo$theoretical_max
```




```{r CompareList}
CompareList <- function (list1, list2) {
  res <- FALSE
  if(length(list1) == length(list2)) {
    res <- length(intersect(list1, list2)) == length(list1)
  }
  res
}
```


```{r set constant}
nVertex <- vcount(facenet) - Op1PercentVertex
```

Let's remove the 0.1% of the vertex with higher **degrees** and check how many components we have.
```{r degrees}
nVertex <- vcount(facenet) - Op1PercentVertex

vertexWithHighestDegrees <- names(sort(degree(facenet), decreasing = TRUE)[1:Op1PercentVertex])
vertexWithHighestDegrees

components.degrees <- components(delete_vertices(facenet, vertexWithHighestDegrees))
components.degrees.no <- components.degrees$no
components.degrees.max <- (max(components.degrees$csize) / nVertex)

components.degrees.no
components.degrees.max
```
We can see that by removing the 0.1% of nodes with the highest degrees, "107", "1684", "1912" and "3437", we can split the network in 41 components.

Let's remove the 0.1% of the vertex with higher **closeness** and check how many components we have.
```{r closeness}
vertexWithHighestCentralCloseness <- names(sort(closeness(facenet), decreasing = TRUE)[1:Op1PercentVertex])
vertexWithHighestCentralCloseness
```
Now compare the 0.1% highest closeness with the list in degrees.
```{r closeness2}
CompareList(vertexWithHighestDegrees, vertexWithHighestCentralCloseness)
intersect(vertexWithHighestDegrees, vertexWithHighestCentralCloseness)
```
# EXP.

Some of the elements are
```{r closeness3}
components.closeness <- components(delete_vertices(facenet, vertexWithHighestCentralCloseness))
components.closeness.no <- components.closeness$no
components.closeness.max <- max(components.closeness$csize) / nVertex

components.closeness.no
components.closeness.max
```
This time by removing vertex "107", "58", "428" and "563" with higher closeness have result in fewer components, 12.

Let's remove the 0.1% of the vertex with higher **betweenness** and check how many components we have.
```{r betweenness}
vertexWithHighestBetweenness <- names(sort(betweenness(facenet), decreasing = TRUE)[1:Op1PercentVertex])
vertexWithHighestBetweenness
```
Now compare the 0.1% highest betweenness with the list in degrees.
```{r betweenness2}
CompareList(vertexWithHighestDegrees, vertexWithHighestBetweenness)

# no need for this results, we already know its value
  #components.betweenness <- components(delete_vertices(facenet, vertexWithHighestBetweenness))
  #components.betweenness.no <- components.betweenness
  #components.betweenness.max <- max(components.betweenness$csize) / nVertex
  #components.betweenness.no # 41
  #components.betweenness.max # 0.9878563
```
Booth lists are equal, then betweenness result is expected to be equal to degrees, 41 components.

Let's remove the 0.1% of the vertex with higher **page_rank score** and check how many components we have.
```{r page_rank}
#names(sort(page_rank(facenet, algo = "power", directed = FALSE)$vector, decreasing = TRUE)[1:Op1PercentVertex]) makes klint to crash ...

# page rank with prpack algorithm
vertexWithHighestPageRank_prpack <- names(sort(page_rank(facenet, algo = "arpack", directed = FALSE)$vector, decreasing = TRUE)[1:Op1PercentVertex])

# page rank with arpack algorithm
vertexWithHighestPageRank_arpack <- names(sort(page_rank(facenet, algo = "prpack", directed = FALSE)$vector, decreasing = TRUE)[1:Op1PercentVertex])

CompareList(vertexWithHighestPageRank_prpack, vertexWithHighestPageRank_arpack)
vertexWithHighestPageRank <- intersect(vertexWithHighestPageRank_prpack, vertexWithHighestPageRank_arpack)
vertexWithHighestPageRank
```
Booth lists are equal.

Now compare the 0.1% highest page_rank scores with the different algorithms, and then with degrees/betweenness and closeness.
```{r page_rank2}
# compare with closeness and betweenness
CompareList(vertexWithHighestDegrees, vertexWithHighestPageRank)
intersect(vertexWithHighestDegrees, vertexWithHighestPageRank)

# compare with closeness
CompareList(vertexWithHighestCentralCloseness, vertexWithHighestPageRank)
intersect(vertexWithHighestCentralCloseness, vertexWithHighestPageRank)
```
There is no difference between algorithms within page_rank in this case. We can do only 1 components run with any of page_rank lists.
Page_rank also selected different vertex than the other 3 methods. Let's calculate its components.
```{r page_rank3}
components.page_rank <- components(delete_vertices(facenet, vertexWithHighestPageRank))
components.page_rank.no <- components.page_rank$no
components.page_rank.max <- max(components.page_rank$csize) / nVertex

components.page_rank.no
components.page_rank.max
```
Page_rank scored the maximum components, 52. This means that page_rank is able to select the most valuable nodes in the newtwork.

To conclude, best was page_rank with 52 components, followed by degrees and betweenness with 41, and last closeness with 12.
I was expecting degrees to be the best one but page_rank proved to be better.

## **2)** Now, consider the same graph as a directed one, and find the hubs and authorities scores. Compare with the page rank score.

```{r generate directed graph}
facenet_directed <- graph_from_data_frame(data, directed = TRUE)
```


#### HUBS and AUTHORITIES

```{r HUBS and AUTHORITIES}
hub_score <- hub_score(facenet_directed, scale=T)$vector
authority_score <- authority_score(facenet_directed, scale =T )$vector
```

```{r}
par(mfrow=c(1,2))
plot(facenet_directed,
     vertex.size=hub_score*30,
     main = "Hubs",
     vertex.color = rainbow(52),
     edge.arrow.size=0.1,
     layout = layout.kamada.kawai,
     vertex.label = NA)

plot(facenet_directed,
     vertex.size=authority_score*30,
     main = "Authority",
     vertex.color = rainbow(52),
     edge.arrow.size=0.1,
     layout = layout.kamada.kawai,
     vertex.label = NA)


#Observamos en los gráficos como las zonas donde hay Hubs y Authority coinciden.

```

```{r HUBS 2}
vertex.hub <- names(sort(hub_score, decreasing = TRUE)[1:Op1PercentVertex])
vertex.hub
```

```{r HUBS 3}
components.hub_score <- components(delete_vertices(facenet_directed, vertex.hub))
components.hub_score.no <- components.hub_score$no
components.hub_score.max <- max(components.hub_score$csize) / nVertex

components.hub_score.no
components.hub_score.max
```

```{r AUTHORITIES 2}
vertex.auth <- names(sort(authority_score, decreasing = TRUE)[1:Op1PercentVertex])
vertex.auth
```

```{r AUTHORITIES 3}
components.authority_score <- components(delete_vertices(facenet_directed, vertex.auth))
components.authority_score.no <- components.authority_score$no
components.authority_score.max <- max(components.authority_score$csize) / nVertex

components.authority_score.no
components.authority_score.max
```

#### Page_rank directed score

```{r Pagerank score}
page_rank <- page_rank(facenet_directed, algo = "arpack", directed = TRUE)$vector # booth algo give same result
```

```{r Pagerank score2}
par(mfrow=c(1,2))
plot(facenet_directed,
     vertex.size=page_rank*30,
     main = "Page rank",
     vertex.color = rainbow(52),
     edge.arrow.size=0.1,
     layout = layout.kamada.kawai,
     vertex.label = NA)

```

```{r page_rank_directed1}
vertex.page_rank <- names(sort(page_rank, decreasing = TRUE)[1:Op1PercentVertex])
vertex.page_rank
```

```{r page_rank_directed2}
components.page_rank <- components(delete_vertices(facenet_directed, vertex.page_rank))
components.page_rank.no <- components.page_rank$no
components.page_rank.max <- max(components.page_rank$csize) / nVertex

components.page_rank.no
components.page_rank.max
```
#### Compare Page_Rank with Hubs and Authorities

- Doubt 1: can we join Hubs and Authorities scores into one list?

```{r compare lists}
CompareList(vertex.auth, vertex.hub)
intersect(vertex.auth, vertex.hub)

CompareList(vertex.auth, vertex.page_rank)
intersect(vertex.auth, vertex.page_rank)

CompareList(vertex.page_rank, vertex.hub)
intersect(vertex.page_rank, vertex.hub)
```
None of the 0.1% high scores are same across the different 3 methods. Furthermore, none has a common element.

Let's compare using previous metrics
```{r compare cluster number}
components.authority_score.no
components.hub_score.no
components.page_rank.no
```
Only hub_score make a difference.

Let's compare the average fraction of the network represented by the largest component.
```{r compare fractions}
components.authority_score.max
components.hub_score.max
components.page_rank.max
```
